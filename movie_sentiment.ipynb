{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "1. [Model1](#model1) - Convolution layers over word2vec \n",
    "2. [Model2](#model2) - RNN over word2vec\n",
    "3. [Model3](#model3) - doc2vec using TF-IDF with word2vec\n",
    "0. [Prediction](#predict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd       \n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import lasagne\n",
    "from lasagne import layers\n",
    "from lasagne.layers import InputLayer,Conv1DLayer,rrelu,\\\n",
    "DenseLayer,ExpressionLayer,MaxPool1DLayer,GRULayer,ConcatLayer,ElemwiseMergeLayer\n",
    "from theano import tensor as tt\n",
    "import theano\n",
    "import matplotlib.pyplot as plt\n",
    "from GBM2 import GBM_KClass\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier,ExtraTreeClassifier,ExtraTreeRegressor\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/home/will/Desktop/data/Movie sentiment/labeledTrainData.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"/home/will/Desktop/data/Movie sentiment/testData.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unlabel = pd.read_csv(\"/home/will/Desktop/data/Movie sentiment/unlabeledTrainData.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load_word2vec_format('/home/will/Desktop/data/Word2Vec/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(review,stop):\n",
    "\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    words = [w for w in words if not w in stops]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/will/anaconda2/lib/python2.7/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file /home/will/anaconda2/lib/python2.7/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "X_str = []\n",
    "for i in range(train.shape[0]):\n",
    "    X_str.append(review_to_wordlist(train.review.iloc[i],stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_str_test = []\n",
    "for i in range(test.shape[0]):\n",
    "    X_str_test.append(review_to_wordlist(test.review.iloc[i],stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = train.sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = y.shape[0]\n",
    "y_np = np.zeros((n,2))\n",
    "y_np[range(n),y]=1\n",
    "y_np=y_np.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_str[:20000]\n",
    "y_train = y_np[:20000]\n",
    "X_val = X_str[20000:]\n",
    "y_val = y_np[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unlabel_str = []\n",
    "for i in range(unlabel.shape[0]):\n",
    "    unlabel_str.append(review_to_wordlist(unlabel.review.iloc[i],stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Batch iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iterator(X, y, WordVec, batchsize, shuffle=False, sublen=1.0):\n",
    "    # inputs is a list of list, targets in np.ndarray\n",
    "    # random \"crop\" of review, with length = int(min(sentence lenth in batch) * sublen)\n",
    "    \n",
    "    n = len(X)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(n)\n",
    "    else:\n",
    "        indices = range(n)\n",
    "\n",
    "    for start_idx in range(0, n - batchsize + 1, batchsize):\n",
    "        length_ = np.zeros(batchsize,dtype='int64')\n",
    "        batch_ = indices[start_idx:start_idx + batchsize]\n",
    "        for i,j in enumerate(batch_): # find the min_len of batch\n",
    "            length_[i] = len(X[j])\n",
    "        min_len = int(length_.min() * sublen)\n",
    "        sample_len = length_ - min_len + 1\n",
    "\n",
    "        for i in range(batchsize): # sample start position\n",
    "            sample_len[i] = np.random.randint(sample_len[i])\n",
    "        \n",
    "        X_batch = np.zeros((batchsize,300,min_len)) # 300 is the word vector's length\n",
    "        \n",
    "        for i in range(batchsize):\n",
    "            for j in range(min_len):\n",
    "                if X[batch_[i]][sample_len[i]+j] in WordVec:\n",
    "                    X_batch[i,:,j] = WordVec[X[batch_[i]][sample_len[i]+j]]\n",
    "\n",
    "        if y==None: # test \n",
    "            yield X_batch.astype('float32'), batch_\n",
    "        else:\n",
    "            yield X_batch.astype('float32'), y[batch_].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iterator_rnn(X, y, WordVec, batchsize, shuffle=False, sublen=1.0):\n",
    "    # inputs is a list of list, targets in np.ndarray\n",
    "    # random \"crop\" of review, with length = int(min(sentence lenth in batch) * sublen)\n",
    "    \n",
    "    n = len(X)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(n)\n",
    "    else:\n",
    "        indices = range(n)\n",
    "\n",
    "    for start_idx in range(0, n - batchsize + 1, batchsize):\n",
    "        length_ = np.zeros(batchsize,dtype='int64')\n",
    "        batch_ = indices[start_idx:start_idx + batchsize]\n",
    "        for i,j in enumerate(batch_): # find the min_len of batch\n",
    "            length_[i] = len(X[j])\n",
    "        min_len = int(length_.min() * sublen)\n",
    "        sample_len = length_ - min_len + 1\n",
    "\n",
    "        for i in range(batchsize): # sample start position\n",
    "            sample_len[i] = np.random.randint(sample_len[i])\n",
    "        \n",
    "        X_batch = np.zeros((batchsize,min_len,300)) # 300 is the word vector's length\n",
    "        \n",
    "        for i in range(batchsize):\n",
    "            for j in range(min_len):\n",
    "                if X[batch_[i]][sample_len[i]+j] in WordVec:\n",
    "                    X_batch[i,j,:] = WordVec[X[batch_[i]][sample_len[i]+j]]\n",
    "        \n",
    "        #if np.random.rand()<flip:\n",
    "        #    X_batch = X_batch[:,::-1,:]\n",
    "            \n",
    "        if y==None: # test \n",
    "            yield X_batch.astype('float32'), batch_\n",
    "        else:\n",
    "            yield X_batch.astype('float32'), y[batch_].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model1'></a>\n",
    "1. Model1 - Conv layer over word2vec from google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tt = tt.tensor3()\n",
    "y_tt = tt.matrix()\n",
    "learning_rate_tt = tt.scalar()\n",
    "m_tt = tt.scalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = InputLayer(shape=(10,300,None),input_var=X_tt)\n",
    "net = rrelu(Conv1DLayer(net,200,5,pad='same'))\n",
    "net = rrelu(Conv1DLayer(net,200,5,pad='same'))\n",
    "#net = rrelu(Conv1DLayer(net,200,3,pad='same'))\n",
    "#net = rrelu(Conv1DLayer(net,200,3,pad='same'))\n",
    "net = ExpressionLayer(net, lambda X: tt.max(X,2),output_shape=(10,200))\n",
    "#net = rrelu(ExpressionLayer(net, lambda X: tt.mean(X,2),output_shape=(10,200)))\n",
    "net = rrelu(DenseLayer(net, num_units=50))\n",
    "net = DenseLayer(net, num_units=2,nonlinearity=tt.nnet.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_hat = layers.get_output(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = -tt.sum(tt.log(Y_hat)*y_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "err = tt.sum(tt.eq(tt.argmax(Y_hat,1),tt.argmax(y_tt,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = lasagne.layers.get_all_params(net, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "            cost, params, learning_rate=learning_rate_tt, momentum=m_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fn = theano.function([X_tt, y_tt,learning_rate_tt,m_tt], updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict  = theano.function([X_tt],Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = theano.function([X_tt, y_tt], err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/will/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:29: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, accuracy:0.7972\n",
      "epoch:1, accuracy:0.7964\n",
      "epoch:2, accuracy:0.7982\n",
      "epoch:3, accuracy:0.7996\n"
     ]
    }
   ],
   "source": [
    "n_batch = 4\n",
    "best_err = .0\n",
    "\n",
    "for i in range(n_batch):\n",
    "    err = 0\n",
    "    \n",
    "    for x_bat,y_bat in batch_iterator(X_train, y_train, model, 10, shuffle=True, sublen=1.0):\n",
    "        train_fn(x_bat,y_bat,2e-5,0.9)\n",
    "    \n",
    "    for x_bat,y_bat in batch_iterator(X_val, y_val, model, 10, shuffle=False, sublen=1.0):\n",
    "        err+=loss(x_bat,y_bat)\n",
    "    \n",
    "    if err > best_err:\n",
    "        best_err = err\n",
    "        best_para = layers.get_all_param_values(net)\n",
    "    \n",
    "    print 'epoch:{}, accuracy:{}'.format(i,1.0*err/y_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers.set_all_param_values(net,best_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model2'></a>\n",
    "Model2 - RNN over word2vec from google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tt = tt.tensor3()\n",
    "y_tt = tt.matrix()\n",
    "learning_rate_tt = tt.scalar()\n",
    "m_tt = tt.scalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_in = InputLayer(shape=(10,None,300),input_var=X_tt)\n",
    "# first layer \n",
    "net_fw = GRULayer(l_in,150,grad_clipping=100.,\\\n",
    "        hidden_update=lasagne.layers.Gate(W_cell=None, nonlinearity=lasagne.nonlinearities.very_leaky_rectify))\n",
    "net_bw = GRULayer(l_in,150,grad_clipping=100.,backwards=True,\\\n",
    "        hidden_update=lasagne.layers.Gate(W_cell=None, nonlinearity=lasagne.nonlinearities.very_leaky_rectify))\n",
    "#net = ElemwiseMergeLayer([net_fw,net_bw],merge_function=tt.maximum)\n",
    "net = rrelu(ElemwiseMergeLayer([net_fw,net_bw],merge_function=tt.add))\n",
    "# second layer \n",
    "net_fw = GRULayer(net,150,grad_clipping=100.,\\\n",
    "        hidden_update=lasagne.layers.Gate(W_cell=None, nonlinearity=lasagne.nonlinearities.very_leaky_rectify))\n",
    "net_bw = GRULayer(net,150,grad_clipping=100.,backwards=True,\\\n",
    "        hidden_update=lasagne.layers.Gate(W_cell=None, nonlinearity=lasagne.nonlinearities.very_leaky_rectify))\n",
    "#net = ElemwiseMergeLayer([net_fw,net_bw],merge_function=tt.maximum)\n",
    "net = rrelu(ElemwiseMergeLayer([net_fw,net_bw],merge_function=tt.add))\n",
    "# third layer \n",
    "#net_fw = GRULayer(net,150,grad_clipping=100.,\\\n",
    "#        hidden_update=lasagne.layers.Gate(W_cell=None, nonlinearity=lasagne.nonlinearities.very_leaky_rectify))\n",
    "#net_bw = GRULayer(net,150,grad_clipping=100.,backwards=True,\\\n",
    "#        hidden_update=lasagne.layers.Gate(W_cell=None, nonlinearity=lasagne.nonlinearities.very_leaky_rectify))\n",
    "#net = ElemwiseMergeLayer([net_fw,net_bw],merge_function=tt.maximum)\n",
    "\n",
    "net = rrelu(ExpressionLayer(net, lambda X: tt.sum(X,1),output_shape=(10,150)))\n",
    "#net = ExpressionLayer(net, lambda X: tt.max(X,1),output_shape=(10,150))\n",
    "net = rrelu(DenseLayer(net, num_units=50))\n",
    "net = DenseLayer(net, num_units=2,nonlinearity=tt.nnet.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_hat = layers.get_output(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = -tt.sum(tt.log(Y_hat)*y_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "err = tt.sum(tt.eq(tt.argmax(Y_hat,1),tt.argmax(y_tt,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = lasagne.layers.get_all_params(net, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "            cost, params, learning_rate=learning_rate_tt, momentum=m_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fn = theano.function([X_tt, y_tt,learning_rate_tt,m_tt], updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict  = theano.function([X_tt],Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = theano.function([X_tt, y_tt], err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/will/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:32: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, accuracy:0.7652\n",
      "epoch:1, accuracy:0.7762\n",
      "epoch:2, accuracy:0.7502\n",
      "epoch:3, accuracy:0.7836\n"
     ]
    }
   ],
   "source": [
    "n_batch = 4\n",
    "\n",
    "for i in range(n_batch):\n",
    "    err = 0\n",
    "    \n",
    "    for x_bat,y_bat in batch_iterator_rnn(X_train, y_train, model, 10, shuffle=True, sublen=1.0):\n",
    "        train_fn(x_bat,y_bat,9e-5,0.95)\n",
    "    \n",
    "    for x_bat,y_bat in batch_iterator_rnn(X_val, y_val, model, 10, shuffle=False, sublen=1.0):\n",
    "        err+=loss(x_bat,y_bat)\n",
    "        \n",
    "    print 'epoch:{}, accuracy:{}'.format(i,1.0*err/y_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='model3'></a>\n",
    "Model3 - doc2vec using TF-IDF with word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_frq = np.zeros((25000,300),dtype='float32')\n",
    "test_frq = np.zeros((25000,300),dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(25000):\n",
    "    temp = np.zeros(300,dtype='float32')\n",
    "    for j in range(len(X_str[i])):\n",
    "        if X_str[i][j] in model:\n",
    "            temp += model[X_str[i][j]]\n",
    "    train_frq[i] = temp/len(X_str[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in xrange(25000):\n",
    "    temp = np.zeros(300,dtype='float32')\n",
    "    for j in range(len(X_str_test[i])):\n",
    "        if X_str_test[i][j] in model:\n",
    "            temp += model[X_str_test[i][j]]\n",
    "    test_frq[i] = temp/len(X_str_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model1=GBM_KClass(ExtraTreeRegressor,30,1e-5,\\\n",
    "{'max_depth':32,'splitter':'random','max_features':0.2},25,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GBM_KClass(BaseEst=<class 'sklearn.tree.tree.ExtraTreeRegressor'>,\n",
       "      BasePara={'max_features': 0.2, 'splitter': 'random', 'max_depth': 32},\n",
       "      M_est=750, learnRate=1e-05, subClass=None, subFold=25)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(train_frq,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='predict'></a>\n",
    "Model Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test = np.zeros((len(X_str_test),2))\n",
    "for _ in range(10):\n",
    "    for x_bat,bat in batch_iterator(X_str_test, None, model, 10, shuffle=True, sublen=1.0):\n",
    "        y_test[bat] = y_test[bat] +  predict(x_bat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test['sentiment'] = np.argmax(y_test,1)\n",
    "test['id'] = test.id.str.replace('\"','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test['sentiment'] = model1.predict(test_frq)\n",
    "test['id'] = test.id.str.replace('\"','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.drop('review',1)\\\n",
    "    .to_csv(r'/home/will/Desktop/data/Movie sentiment/model1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
