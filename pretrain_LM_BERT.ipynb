{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset,DataLoader,RandomSampler,SequentialSampler,TensorDataset\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification,BertForMaskedLM\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from pytorch_util import *\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_train_epochs = 4\n",
    "max_len = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Data/precessed_reviews.pickle', 'rb') as fp:\n",
    "    precessed_reviews = pickle.load(fp)\n",
    "with open('../Data/targets.pickle', 'rb') as fp:\n",
    "    targets = pickle.load(fp) \n",
    "with open('../Data/precessed_reviews_test.pickle', 'rb') as fp:\n",
    "    precessed_reviews_test = pickle.load(fp)  \n",
    "with open('../Data/precessed_reviews_unlabeled.pickle', 'rb') as fp:\n",
    "    precessed_reviews_unlabeled = pickle.load(fp)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM_text = precessed_reviews + precessed_reviews_test + precessed_reviews_unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(Dataset):\n",
    "    def __init__(self, reviews,targets,max_len=64):\n",
    "        self.reviews = reviews\n",
    "        self.targets = targets\n",
    "        self.max_len = max_len\n",
    "        self.length = [len(r)+2 for r in reviews]#+2 for CLS and SEP\n",
    "        self.segment_ids = np.zeros(max_len,dtype=np.long)\n",
    "        self.CLS = [101]\n",
    "        self.SEP = [102]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        review = self.reviews[index]\n",
    "        len_ = self.length[index]\n",
    "        if len_ < self.max_len:\n",
    "            padding = [0] * (self.max_len - len_)\n",
    "            input_mask = [1] * len_\n",
    "            input_ids = self.CLS + review + self.SEP + padding\n",
    "            input_mask += padding\n",
    "        elif len_ == self.max_len:\n",
    "            input_ids = self.CLS + review + self.SEP\n",
    "            input_mask = [1] * len_\n",
    "        else:\n",
    "            start = np.random.randint(0,len_-self.max_len+1)\n",
    "            input_ids = self.CLS + review[start:start+self.max_len-2] + self.SEP\n",
    "            input_mask = [1] * self.max_len\n",
    "        return np.array(input_ids,dtype=np.long),np.array(input_mask,dtype=np.long),\\\n",
    "                self.segment_ids,self.targets[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_word(tokens):\n",
    "    MASK = 103\n",
    "    output_label = []\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        prob = np.random.rand()\n",
    "        # mask token with 15% probability\n",
    "        if prob < 0.15:\n",
    "            prob = prob/0.15\n",
    "            \n",
    "            # append current token to output (we will predict these later)\n",
    "            output_label.append(token)\n",
    "            \n",
    "            # 80% randomly change token to mask token\n",
    "            if prob < 0.8:\n",
    "                tokens[i] = MASK\n",
    "\n",
    "            # 10% randomly change token to random token, token goes from 0 to 30521\n",
    "            elif prob < 0.9:\n",
    "                tokens[i] = np.random.randint(0,30522)\n",
    "\n",
    "            # -> rest 10% randomly keep current token\n",
    "        else:\n",
    "            # no masking token (will be ignored by loss function later)\n",
    "            output_label.append(-1)\n",
    "\n",
    "    return tokens, output_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator_LM(Dataset):\n",
    "    def __init__(self, reviews,max_len=64):\n",
    "        self.reviews = reviews\n",
    "        self.max_len = max_len\n",
    "        self.length = [len(r)+2 for r in reviews]#+2 for CLS and SEP\n",
    "        self.segment_ids = np.zeros(max_len,dtype=np.long)\n",
    "        self.CLS = [101]\n",
    "        self.SEP = [102]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        review = self.reviews[index]\n",
    "        len_ = self.length[index]\n",
    "        if len_ < self.max_len:\n",
    "            review, target = random_word(review)\n",
    "            padding = [0] * (self.max_len - len_)\n",
    "            input_mask = [1] * len_\n",
    "            input_ids = self.CLS + review + self.SEP + padding\n",
    "            target = [-1] + target + [-1]*(self.max_len - len_ + 1)\n",
    "            input_mask += padding\n",
    "        elif len_ == self.max_len:\n",
    "            review, target = random_word(review)\n",
    "            input_ids = self.CLS + review + self.SEP\n",
    "            target = [-1] + target + [-1]\n",
    "            input_mask = [1] * len_\n",
    "        else:\n",
    "            start = np.random.randint(0,len_-self.max_len+1)\n",
    "            review, target = random_word(review[start:start+self.max_len-2])\n",
    "            input_ids = self.CLS + review + self.SEP\n",
    "            target = [-1] + target + [-1]\n",
    "            input_mask = [1] * self.max_len\n",
    "        return np.array(input_ids,dtype=np.long),np.array(input_mask,dtype=np.long),\\\n",
    "                self.segment_ids,np.array(target,dtype=np.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = 20000\n",
    "# train_gen = TextGenerator(precessed_reviews[:train_size],targets[:train_size],max_len)\n",
    "# train_gen = DataLoader(train_gen,batch_size,sampler=RandomSampler(train_gen),num_workers=2)\n",
    "\n",
    "# val_gen = TextGenerator(precessed_reviews[train_size:],targets[train_size:],max_len)\n",
    "# val_gen = DataLoader(val_gen,batch_size,sampler=SequentialSampler(train_gen),num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with all dataset for better test performance\n",
    "train_gen = TextGenerator_LM(LM_text,max_len)\n",
    "train_gen = DataLoader(train_gen,batch_size,sampler=RandomSampler(train_gen),num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs,inputs_mask,segment_ids,ys = next(iter(train_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased').to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_optimization_steps = int(len(train_gen) * num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=5e-5,\n",
    "                     warmup=0.1,\n",
    "                     t_total=num_train_optimization_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(model,data):\n",
    "    inputs,inputs_mask,segment_ids,ys = data\n",
    "    loss = model(inputs,segment_ids,inputs_mask,ys)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:2.990750481300354\n",
      "epoch:1, train_loss:2.831921937408447\n",
      "epoch:2, train_loss:2.7265457973861693\n",
      "epoch:3, train_loss:2.6425181249427796\n",
      "Training completed in 4677.281453609467s\n"
     ]
    }
   ],
   "source": [
    "# model = fit(num_train_epochs, model, loss_func, opt, train_gen, val_gen)\n",
    "model = fit(num_train_epochs, model, loss_func, opt, train_gen, lossBest=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '../Model/BertForMaskedLM.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.bert, '../Model/BertBase.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = TextGenerator(precessed_reviews,targets,max_len)\n",
    "train_gen = DataLoader(train_gen,batch_size,sampler=RandomSampler(train_gen),num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft.bert = model.bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_optimization_steps = int(len(train_gen) * num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=5e-5,\n",
    "                     warmup=0.1,\n",
    "                     t_total=num_train_optimization_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train_loss:0.4091405396906614\n",
      "epoch:1, train_loss:0.318999502600365\n",
      "epoch:2, train_loss:0.26345209118632285\n",
      "epoch:3, train_loss:0.21854930106555226\n",
      "Training completed in 1039.646898984909s\n"
     ]
    }
   ],
   "source": [
    "model_ft = fit(num_train_epochs, model_ft, loss_func, opt, train_gen, lossBest=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchedSeq(reviews,max_len,window_len=32):\n",
    "    # used for TTA\n",
    "    CLS = [101]\n",
    "    SEP = [102]\n",
    "    length = [len(i)+2 for i in reviews]\n",
    "    input_ids = []\n",
    "    input_masks = []\n",
    "    seg_len = []\n",
    "    for len_,review in zip(length,reviews):\n",
    "        if len_ < max_len:\n",
    "            padding = [0] * (max_len - len_)\n",
    "            input_mask = [1] * len_\n",
    "            input_id = CLS + review + SEP + padding\n",
    "            input_mask += padding\n",
    "            input_ids.append(input_id)\n",
    "            input_masks.append(input_mask)\n",
    "            seg_len.append(1)\n",
    "        elif len_ == max_len:\n",
    "            input_id = CLS + review + SEP\n",
    "            input_mask = [1] * len_\n",
    "            input_ids.append(input_id)\n",
    "            input_masks.append(input_mask)\n",
    "            seg_len.append(1)\n",
    "        else:\n",
    "            _len_seg = (len_ - max_len)//window_len + 1\n",
    "            for j in range(_len_seg):\n",
    "                input_id = CLS + review[j*window_len:j*window_len+(max_len-2)] + SEP\n",
    "                input_mask = [1] * max_len\n",
    "                input_ids.append(input_id)\n",
    "                input_masks.append(input_mask)\n",
    "            seg_len.append(_len_seg)\n",
    "            \n",
    "    return np.array(input_ids,dtype=np.long),np.array(input_masks,dtype=np.long),\\\n",
    "                np.zeros((len(input_ids),max_len),dtype=np.long),seg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs,inputs_mask,segment_ids,seg_len = batchedSeq(precessed_reviews[train_size:],max_len)\n",
    "TTA_val_gen = TensorDataset(*[torch.tensor(i) for i in [inputs,segment_ids,inputs_mask]])\n",
    "TTA_val_gen = DataLoader(TTA_val_gen,batch_size,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model,dataloader,to_numpy=True):\n",
    "    # dataloader return Xs only\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = torch.cat([model(*data2cuda(data)) for data in dataloader])\n",
    "        return out.cpu().detach().numpy() if to_numpy else out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "yhat = predict(model,TTA_val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_len = np.cumsum(np.array(seg_len))\n",
    "seg_len = np.insert(seg_len, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_agg = []\n",
    "for i in range(seg_len.shape[0]-1):\n",
    "    yhat_agg.append(yhat[seg_len[i]:seg_len[i+1]].mean(0))\n",
    "yhat_agg = np.array(yhat_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9132"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation accuracy\n",
    "np.sum(yhat_agg.argmax(1) == np.array(targets[train_size:]))/yhat_agg.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_prob = np.exp(yhat)\n",
    "yhat_prob = yhat_prob/yhat_prob.sum(1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_agg2 = []\n",
    "for i in range(seg_len.shape[0]-1):\n",
    "    yhat_agg2.append(yhat_prob[seg_len[i]:seg_len[i+1]].mean(0))\n",
    "yhat_agg2 = np.array(yhat_agg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9096"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation accuracy\n",
    "np.sum(yhat_agg2.argmax(1) == np.array(targets[train_size:]))/yhat_agg.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9638380238417107"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AUC\n",
    "roc_auc_score(np.array(targets[train_size:]),yhat_agg2[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../Data/sampleSubmission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs,inputs_mask,segment_ids,seg_len = batchedSeq(precessed_reviews_test,max_len)\n",
    "TTA_val_gen = TensorDataset(*[torch.tensor(i) for i in [inputs,segment_ids,inputs_mask]])\n",
    "TTA_val_gen = DataLoader(TTA_val_gen,batch_size,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _DataLoaderIter.__del__ at 0x7f5e8372fb70>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/will/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/will/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/will/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <function _DataLoaderIter.__del__ at 0x7f5e8372fb70>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/will/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/will/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/will/anaconda3/envs/pytorch/lib/python3.7/multiprocessing/process.py\", line 138, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n"
     ]
    }
   ],
   "source": [
    "ytest = predict(model_ft,TTA_val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_len = np.cumsum(np.array(seg_len))\n",
    "seg_len = np.insert(seg_len, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_agg = []\n",
    "for i in range(seg_len.shape[0]-1):\n",
    "    yhat_agg.append(ytest[seg_len[i]:seg_len[i+1]].mean(0))\n",
    "yhat_agg = np.array(yhat_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_prob = np.exp(yhat_agg)\n",
    "yhat_prob = yhat_prob/yhat_prob.sum(1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.iloc[:,1] = yhat_prob[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('../Submission/BERT_ft.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
