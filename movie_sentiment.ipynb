{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index\n",
    "1. [Model1](#model1) - Convolution layers over word2vec \n",
    "2. [Model2](#model2) - RNN over word2vec\n",
    "3. [Model3](#model3) - doc2vec using TF-IDF with word2vec\n",
    "0. [Prediction](#predict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd       \n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import lasagne\n",
    "from lasagne import layers\n",
    "from lasagne.layers import InputLayer,Conv1DLayer,rrelu,prelu,batch_norm,\\\n",
    "DenseLayer,ExpressionLayer,MaxPool1DLayer,GRULayer,ConcatLayer,ElemwiseMergeLayer,Gate\n",
    "from lasagne.updates import nesterov_momentum,adam\n",
    "from lasagne.nonlinearities import very_leaky_rectify,tanh\n",
    "from theano import tensor as tt\n",
    "import theano\n",
    "import matplotlib.pyplot as plt\n",
    "from GBM2 import GBM_KClass\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier,ExtraTreeClassifier,ExtraTreeRegressor\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/home/will/Desktop/data/Movie sentiment/labeledTrainData.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"/home/will/Desktop/data/Movie sentiment/testData.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "unlabel = pd.read_csv(\"/home/will/Desktop/data/Movie sentiment/unlabeledTrainData.tsv\", header=0, \\\n",
    "                    delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load_word2vec_format('/home/will/Desktop/data/Word2Vec/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(review,stop):\n",
    "\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    words = [w for w in words if not w in stops]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_str = []\n",
    "for i in range(train.shape[0]):\n",
    "    X_str.append(review_to_wordlist(train.review.iloc[i],stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_str_test = []\n",
    "for i in range(test.shape[0]):\n",
    "    X_str_test.append(review_to_wordlist(test.review.iloc[i],stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = train.sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = y.shape[0]\n",
    "y_np = np.zeros((n,2))\n",
    "y_np[range(n),y]=1\n",
    "y_np=y_np.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_str[:20000]\n",
    "y_train = y_np[:20000]\n",
    "X_val = X_str[20000:]\n",
    "y_val = y_np[20000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "unlabel_str = []\n",
    "for i in range(unlabel.shape[0]):\n",
    "    unlabel_str.append(review_to_wordlist(unlabel.review.iloc[i],stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Batch iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iterator(X, y, WordVec, batchsize, shuffle=False, sublen=1.0):\n",
    "    # inputs is a list of list, targets in np.ndarray\n",
    "    # random \"crop\" of review, with length = int(min(sentence lenth in batch) * sublen)\n",
    "    \n",
    "    n = len(X)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(n)\n",
    "    else:\n",
    "        indices = range(n)\n",
    "\n",
    "    for start_idx in range(0, n - batchsize + 1, batchsize):\n",
    "        length_ = np.zeros(batchsize,dtype='int64')\n",
    "        batch_ = indices[start_idx:start_idx + batchsize]\n",
    "        for i,j in enumerate(batch_): # find the min_len of batch\n",
    "            length_[i] = len(X[j])\n",
    "        min_len = int(length_.min() * sublen)\n",
    "        sample_len = length_ - min_len + 1\n",
    "\n",
    "        for i in range(batchsize): # sample start position\n",
    "            sample_len[i] = np.random.randint(sample_len[i])\n",
    "        \n",
    "        X_batch = np.zeros((batchsize,300,min_len)) # 300 is the word vector's length\n",
    "        \n",
    "        for i in range(batchsize):\n",
    "            for j in range(min_len):\n",
    "                if X[batch_[i]][sample_len[i]+j] in WordVec:\n",
    "                    X_batch[i,:,j] = WordVec[X[batch_[i]][sample_len[i]+j]]\n",
    "\n",
    "        if y==None: # test \n",
    "            yield X_batch.astype('float32'), batch_\n",
    "        else:\n",
    "            yield X_batch.astype('float32'), y[batch_].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iterator_rnn(X, y, WordVec, batchsize, shuffle=False, sublen=1.0):\n",
    "    # inputs is a list of list, targets in np.ndarray\n",
    "    # random \"crop\" of review, with length = int(min(sentence lenth in batch) * sublen)\n",
    "    \n",
    "    n = len(X)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(n)\n",
    "    else:\n",
    "        indices = range(n)\n",
    "\n",
    "    for start_idx in range(0, n - batchsize + 1, batchsize):\n",
    "        length_ = np.zeros(batchsize,dtype='int64')\n",
    "        batch_ = indices[start_idx:start_idx + batchsize]\n",
    "        for i,j in enumerate(batch_): # find the min_len of batch\n",
    "            length_[i] = len(X[j])\n",
    "        min_len = int(length_.min() * sublen)\n",
    "        sample_len = length_ - min_len + 1\n",
    "\n",
    "        for i in range(batchsize): # sample start position\n",
    "            sample_len[i] = np.random.randint(sample_len[i])\n",
    "        \n",
    "        X_batch = np.zeros((batchsize,min_len,300)) # 300 is the word vector's length\n",
    "        \n",
    "        for i in range(batchsize):\n",
    "            for j in range(min_len):\n",
    "                if X[batch_[i]][sample_len[i]+j] in WordVec:\n",
    "                    X_batch[i,j,:] = WordVec[X[batch_[i]][sample_len[i]+j]]\n",
    "        \n",
    "        #if np.random.rand()<flip:\n",
    "        #    X_batch = X_batch[:,::-1,:]\n",
    "            \n",
    "        if y==None: # test \n",
    "            yield X_batch.astype('float32'), batch_\n",
    "        else:\n",
    "            yield X_batch.astype('float32'), y[batch_].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_fun(batch_size, n_layer,n_filter,filter_size,connect_layer,ExpressFun,n_dense,\\\n",
    "               update_dic):\n",
    "    \n",
    "    update_method = update_dic['mtd']\n",
    "    update_para = update_dic['para']\n",
    "    \n",
    "    X_tt = tt.tensor3()\n",
    "    y_tt = tt.matrix()\n",
    "    \n",
    "    net = InputLayer(shape=(batch_size,300,None),input_var=X_tt)\n",
    "    \n",
    "    for _ in range(n_layer):\n",
    "        net = connect_layer(Conv1DLayer(net,n_filter,filter_size,pad='same'))\n",
    "\n",
    "    net = ExpressionLayer(net, lambda X: ExpressFun(X,2),output_shape=(batch_size,n_filter))\n",
    "    net = connect_layer(DenseLayer(net, num_units=n_dense))\n",
    "    net = DenseLayer(net, num_units=2,nonlinearity=tt.nnet.softmax)\n",
    "    \n",
    "    Y_hat = layers.get_output(net)\n",
    "    Y_hat_test = layers.get_output(net,deterministic=True)\n",
    "    \n",
    "    cost = -tt.sum(tt.log(Y_hat)*y_tt)\n",
    "    params = lasagne.layers.get_all_params(net, trainable=True)\n",
    "    updates = update_method(cost, params, **update_para)\n",
    "    train_fn = theano.function([X_tt, y_tt], updates=updates)\n",
    "    predict  = theano.function([X_tt],Y_hat_test)\n",
    "    \n",
    "    return train_fn, predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperPara(params):\n",
    "    \n",
    "    batch_size=int(params[0])\n",
    "    n_layer=int(params[1])\n",
    "    n_filter=int(params[2])\n",
    "    filter_size=int(params[3])\n",
    "    connect_layer=params[4]\n",
    "    ExpressFun=params[5]\n",
    "    n_dense=int(params[6])\n",
    "    update_dic=params[7]\n",
    "    \n",
    "    n_batch = 30\n",
    "    best_score = .0\n",
    "    counter = 0\n",
    "    yhat = np.zeros((len(X_val),2))\n",
    "    \n",
    "    train_fn, predict = create_fun(batch_size, n_layer,n_filter,filter_size,connect_layer,ExpressFun,n_dense,\\\n",
    "                                   update_dic)\n",
    "    \n",
    "    for i in range(n_batch):\n",
    "\n",
    "        for x_bat,y_bat in batch_iterator(X_train, y_train, model, batch_size, shuffle=True, sublen=1.0):\n",
    "            train_fn(x_bat,y_bat)\n",
    "\n",
    "        for x_bat,batch in batch_iterator(X_val, None, model, batch_size, shuffle=False, sublen=1.0):\n",
    "            yhat[batch]=predict(x_bat)\n",
    "\n",
    "        score_ = roc_auc_score(np.argmax(y_val,1),yhat[:,1])\n",
    "        \n",
    "        if score_ > best_score:\n",
    "            best_score = score_\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter+=1\n",
    "            if counter > 2 and i > 10: # early stop\n",
    "                break \n",
    "        \n",
    "        \n",
    "    print 'batch_size:{}, n_layer:{},n_filter:{},filter_size:{},connect_layer:{},ExpressFun:{},n_dense:{},\\\n",
    "              update_method:{}, update_para:{}, accuracy:{}'\\\n",
    "    .format(batch_size, n_layer,n_filter,filter_size,connect_layer,ExpressFun,n_dense,\\\n",
    "              update_dic['mtd'], update_dic['para'],best_score)    \n",
    "    \n",
    "    return -best_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "space =[hp.quniform('batch_size',10,40,5),\\\n",
    "        hp.quniform('n_layer',1,5,1),\\\n",
    "        hp.quniform('n_filter',100,500,100),\\\n",
    "        hp.quniform('filter_size',3,10,2) - 1,\\\n",
    "        hp.choice('connect_layer',[rrelu,prelu,batch_norm]),\\\n",
    "        hp.choice('ExpressFun',[tt.max, tt.mean]),\\\n",
    "        hp.quniform('n_dense',50,300,50),\\\n",
    "        hp.choice('update_method',\\\n",
    "                  [{'mtd':nesterov_momentum, 'para':{'learning_rate':2e-5*2**(-hp.quniform('nest_r',1,5,1)),\\\n",
    "                                                     'momentum':hp.uniform('nest_mom',0.6,0.99)}},\\\n",
    "                   {'mtd':adam, 'para':{'learning_rate':2e-5*2**(-hp.quniform('ada_r',1,5,1))}}\n",
    "                  ])\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best = fmin(hyperPara, space, algo=tpe.suggest, trials=trials, max_evals=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model1'></a>\n",
    "1. Model1 - Conv layer over word2vec from google, with optimized hyperpara from hyperopt over whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tt = tt.tensor3()\n",
    "y_tt = tt.matrix()\n",
    "learning_rate_tt = tt.scalar()\n",
    "m_tt = tt.scalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "net = InputLayer(shape=(10,300,None),input_var=X_tt)\n",
    "net = rrelu(Conv1DLayer(net,200,5,pad='same'))\n",
    "net = rrelu(Conv1DLayer(net,200,5,pad='same'))\n",
    "net = ExpressionLayer(net, lambda X: tt.max(X,2),output_shape=(10,200))\n",
    "#net = rrelu(ExpressionLayer(net, lambda X: tt.mean(X,2),output_shape=(10,200)))\n",
    "#net = rrelu(DenseLayer(net, num_units=100))\n",
    "net = rrelu(DenseLayer(net, num_units=50))\n",
    "net = DenseLayer(net, num_units=2,nonlinearity=tt.nnet.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_hat = layers.get_output(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = -tt.sum(tt.log(Y_hat)*y_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "err = tt.sum(tt.eq(tt.argmax(Y_hat,1),tt.argmax(y_tt,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = lasagne.layers.get_all_params(net, trainable=True)\n",
    "updates = adam(cost, params, learning_rate=learning_rate_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fn = theano.function([X_tt, y_tt,learning_rate_tt], updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict  = theano.function([X_tt],Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = theano.function([X_tt, y_tt], err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whole dataset refit - final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_batch = 20\n",
    "\n",
    "for i in range(n_batch):\n",
    "    \n",
    "    for x_bat,y_bat in batch_iterator(X_str, y_np, model, 10, shuffle=True, sublen=1.0):\n",
    "        train_fn(x_bat,y_bat,1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "n_batch = 10\n",
    "best_err = .0\n",
    "yhat = np.zeros((len(X_val),2))\n",
    "\n",
    "for i in range(n_batch):\n",
    "    \n",
    "    for x_bat,y_bat in batch_iterator(X_train, y_train, model, 10, shuffle=True, sublen=1.0):\n",
    "        train_fn(x_bat,y_bat,1e-5)\n",
    "    \n",
    "    for x_bat,batch in batch_iterator(X_val, None, model, 10, shuffle=False, sublen=1.0):\n",
    "        yhat[batch]=predict(x_bat)\n",
    "\n",
    "    err = roc_auc_score(np.argmax(y_val,1),yhat[:,1])\n",
    "    if err > best_err:\n",
    "        best_err = err\n",
    "        best_para = layers.get_all_param_values(net)\n",
    "    \n",
    "    print 'epoch:{}, accuracy:{}'.format(i,err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "layers.set_all_param_values(net,best_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_fun_RNN(n_layer,n_batch,n_unit,cell1,cell2,cell3,nonlin1,connect1,fun1,fun2,n_dense,\\\n",
    "               update_dic):\n",
    "    \n",
    "    update_method = update_dic['mtd']\n",
    "    update_para = update_dic['para']\n",
    "    \n",
    "    X_tt = tt.tensor3()\n",
    "    y_tt = tt.matrix()\n",
    "    \n",
    "    net = InputLayer(shape=(n_batch,None,300),input_var=X_tt)\n",
    "\n",
    "    for _ in range(n_layer):\n",
    "        net_fw = GRULayer(net,n_unit,grad_clipping=100.,\\\n",
    "                            resetgate=Gate(W_cell=cell1),\\\n",
    "                            updategate=Gate(W_cell=cell2),\\\n",
    "                            hidden_update=Gate(W_cell=cell3, nonlinearity=nonlin1))\n",
    "        net_bw = GRULayer(net,n_unit,grad_clipping=100.,backwards=True,\\\n",
    "                            resetgate=Gate(W_cell=cell1),\\\n",
    "                            updategate=Gate(W_cell=cell2),\\\n",
    "                            hidden_update=Gate(W_cell=cell3, nonlinearity=nonlin1))\n",
    "        \n",
    "        if connect1 == batch_norm:\n",
    "            net = connect1(ElemwiseMergeLayer([net_fw,net_bw],merge_function=fun1),axes=(0,1))\n",
    "        elif connect1 == \"identity\":\n",
    "            net = ElemwiseMergeLayer([net_fw,net_bw],merge_function=fun1)\n",
    "        else:\n",
    "            net = connect1(ElemwiseMergeLayer([net_fw,net_bw],merge_function=fun1),shared_axes=(0,1))\n",
    "    \n",
    "            \n",
    "    if connect1 == batch_norm:\n",
    "        net = connect1(ExpressionLayer(net, lambda X: fun2(X,1),output_shape=(n_batch,n_unit)),axes=(0,1))\n",
    "    elif connect1 == \"identity\":\n",
    "        net = ExpressionLayer(net, lambda X: fun2(X,1),output_shape=(n_batch,n_unit))\n",
    "    else:\n",
    "        net = connect1(ExpressionLayer(net, lambda X: fun2(X,1),output_shape=(n_batch,n_unit)),shared_axes=(0,1))\n",
    "        \n",
    "        \n",
    "    if connect1 == \"identity\":\n",
    "        net = DenseLayer(net, num_units=n_dense)\n",
    "    else:\n",
    "        net = connect1(DenseLayer(net, num_units=n_dense))\n",
    "        \n",
    "    net = DenseLayer(net, num_units=2,nonlinearity=tt.nnet.softmax)\n",
    "    \n",
    "    Y_hat = layers.get_output(net)\n",
    "    Y_hat_test = layers.get_output(net,deterministic=True)\n",
    "    \n",
    "    cost = -tt.sum(tt.log(Y_hat)*y_tt)\n",
    "    params = lasagne.layers.get_all_params(net, trainable=True)\n",
    "    updates = update_method(cost, params, **update_para)\n",
    "    train_fn = theano.function([X_tt, y_tt], updates=updates)\n",
    "    predict  = theano.function([X_tt],Y_hat_test)\n",
    "    \n",
    "    return train_fn, predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperPara_RNN(params):\n",
    "    \n",
    "    n_layer=int(params[0])\n",
    "    batch_size=int(params[1])\n",
    "    n_unit=int(params[2])\n",
    "    cell1=params[3]\n",
    "    cell2=params[4]\n",
    "    cell3=params[5]\n",
    "    nonlin1=params[6]\n",
    "    connect1=params[7]\n",
    "    fun1=params[8]\n",
    "    fun2=params[9]\n",
    "    n_dense=int(params[10])\n",
    "    update_dic=params[11]\n",
    "\n",
    "\n",
    "    n_batch = 30\n",
    "    best_score = .0\n",
    "    counter = 0\n",
    "    yhat = np.zeros((len(X_val),2))\n",
    "    \n",
    "    train_fn, predict = create_fun_RNN(n_layer,batch_size,n_unit,cell1,cell2,cell3,nonlin1,connect1,\\\n",
    "                                       fun1,fun2,n_dense,update_dic)\n",
    "    \n",
    "    for i in range(n_batch):\n",
    "\n",
    "        for x_bat,y_bat in batch_iterator_rnn(X_train, y_train, model, batch_size, shuffle=True, sublen=1.0):\n",
    "            train_fn(x_bat,y_bat)\n",
    "\n",
    "        for x_bat,batch in batch_iterator_rnn(X_val, None, model, batch_size, shuffle=False, sublen=1.0):\n",
    "            yhat[batch]=predict(x_bat)\n",
    "\n",
    "        score_ = roc_auc_score(np.argmax(y_val,1),yhat[:,1])\n",
    "        \n",
    "        if score_ > best_score:\n",
    "            best_score = score_\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter+=1\n",
    "            if counter > 2 and i > 10: # early stop\n",
    "                break \n",
    "        \n",
    "        \n",
    "    print 'batch_size:{}, n_layer:{},n_unit:{},cell1:{},cell2:{},cell3:{},\\\n",
    "           nonlin1:{},connect1:{},n_dense:{},fun1:{},fun2:{},\\\n",
    "              update_method:{}, update_para:{},  ### Accuracy:{} ###'\\\n",
    "    .format(batch_size, n_layer,n_unit,cell1,cell2,cell3,nonlin1,connect1,n_dense,fun1,fun2,\\\n",
    "              update_dic['mtd'], update_dic['para'],best_score)    \n",
    "    \n",
    "    return -best_score   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "space =[hp.quniform('n_layer',1,2,1),\\\n",
    "        hp.quniform('batch_size',10,40,5),\\\n",
    "        hp.quniform('n_unit',100,500,100),\\\n",
    "        hp.choice('cell1',[True, False]),\\\n",
    "        hp.choice('cell2',[True, False]),\\\n",
    "        hp.choice('cell3',[True, False]),\\\n",
    "        hp.choice('nonlin1',[very_leaky_rectify,tanh]),\\\n",
    "        hp.choice('connect1',[rrelu,prelu,batch_norm,\"identity\"]),\\\n",
    "        hp.choice('fun1',[tt.add, tt.maximum]),\\\n",
    "        hp.choice('fun2',[tt.sum, tt.max]),\\\n",
    "        hp.quniform('n_dense',50,300,50),\\\n",
    "        hp.choice('update_method',\\\n",
    "                  [{'mtd':nesterov_momentum, 'para':{'learning_rate':4e-5*2**(-hp.quniform('nest_r',1,5,1)),\\\n",
    "                                                     'momentum':hp.uniform('nest_mom',0.7,0.99)}},\\\n",
    "                   {'mtd':adam, 'para':{'learning_rate':4e-5*2**(-hp.quniform('ada_r',1,5,1))}}\n",
    "                  ])\n",
    "        ]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best = fmin(hyperPara_RNN, space, algo=tpe.suggest, trials=trials, max_evals=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model2'></a>\n",
    "Model2 - RNN over word2vec from google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tt = tt.tensor3()\n",
    "y_tt = tt.matrix()\n",
    "learning_rate_tt = tt.scalar()\n",
    "m_tt = tt.scalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_in = InputLayer(shape=(10,None,300),input_var=X_tt)\n",
    "# first layer \n",
    "net_fw = GRULayer(l_in,150,grad_clipping=100.,\\\n",
    "        hidden_update=lasagne.layers.Gate(W_cell=None, nonlinearity=lasagne.nonlinearities.very_leaky_rectify))\n",
    "net_bw = GRULayer(l_in,150,grad_clipping=100.,backwards=True,\\\n",
    "        hidden_update=lasagne.layers.Gate(W_cell=None, nonlinearity=lasagne.nonlinearities.very_leaky_rectify))\n",
    "#net = ElemwiseMergeLayer([net_fw,net_bw],merge_function=tt.maximum)\n",
    "net = rrelu(ElemwiseMergeLayer([net_fw,net_bw],merge_function=tt.add))\n",
    "# second layer \n",
    "net_fw = GRULayer(net,150,grad_clipping=100.,\\\n",
    "        hidden_update=lasagne.layers.Gate(W_cell=None, nonlinearity=lasagne.nonlinearities.very_leaky_rectify))\n",
    "net_bw = GRULayer(net,150,grad_clipping=100.,backwards=True,\\\n",
    "        hidden_update=lasagne.layers.Gate(W_cell=None, nonlinearity=lasagne.nonlinearities.very_leaky_rectify))\n",
    "#net = ElemwiseMergeLayer([net_fw,net_bw],merge_function=tt.maximum)\n",
    "net = rrelu(ElemwiseMergeLayer([net_fw,net_bw],merge_function=tt.add))\n",
    "# third layer \n",
    "#net_fw = GRULayer(net,150,grad_clipping=100.,\\\n",
    "#        hidden_update=lasagne.layers.Gate(W_cell=None, nonlinearity=lasagne.nonlinearities.very_leaky_rectify))\n",
    "#net_bw = GRULayer(net,150,grad_clipping=100.,backwards=True,\\\n",
    "#        hidden_update=lasagne.layers.Gate(W_cell=None, nonlinearity=lasagne.nonlinearities.very_leaky_rectify))\n",
    "#net = ElemwiseMergeLayer([net_fw,net_bw],merge_function=tt.maximum)\n",
    "\n",
    "net = rrelu(ExpressionLayer(net, lambda X: tt.sum(X,1),output_shape=(10,150)))\n",
    "#net = ExpressionLayer(net, lambda X: tt.max(X,1),output_shape=(10,150))\n",
    "net = rrelu(DenseLayer(net, num_units=50))\n",
    "net = DenseLayer(net, num_units=2,nonlinearity=tt.nnet.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_hat = layers.get_output(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = -tt.sum(tt.log(Y_hat)*y_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "err = tt.sum(tt.eq(tt.argmax(Y_hat,1),tt.argmax(y_tt,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = lasagne.layers.get_all_params(net, trainable=True)\n",
    "updates = lasagne.updates.nesterov_momentum(\n",
    "            cost, params, learning_rate=learning_rate_tt, momentum=m_tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fn = theano.function([X_tt, y_tt,learning_rate_tt,m_tt], updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict  = theano.function([X_tt],Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = theano.function([X_tt, y_tt], err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_batch = 4\n",
    "yhat = np.zeros((len(X_val),2))\n",
    "\n",
    "for i in range(n_batch):\n",
    "    err = 0\n",
    "    \n",
    "    for x_bat,y_bat in batch_iterator_rnn(X_train, y_train, model, 10, shuffle=True, sublen=1.0):\n",
    "        train_fn(x_bat,y_bat)\n",
    "    \n",
    "    for x_bat,batch in batch_iterator_rnn(X_val, None, model, 10, shuffle=False, sublen=1.0):\n",
    "        yhat[batch]=predict(x_bat)\n",
    "        \n",
    "    score_ = roc_auc_score(np.argmax(y_val,1),yhat[:,1])\n",
    "    \n",
    "    print 'epoch:{}, accuracy:{}'.format(i,score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='model3'></a>\n",
    "Model3 - doc2vec using TF-IDF with word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_frq = np.zeros((25000,300),dtype='float32')\n",
    "test_frq = np.zeros((25000,300),dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in xrange(25000):\n",
    "    temp = np.zeros(300,dtype='float32')\n",
    "    for j in range(len(X_str[i])):\n",
    "        if X_str[i][j] in model:\n",
    "            temp += model[X_str[i][j]]\n",
    "    train_frq[i] = temp/len(X_str[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in xrange(25000):\n",
    "    temp = np.zeros(300,dtype='float32')\n",
    "    for j in range(len(X_str_test[i])):\n",
    "        if X_str_test[i][j] in model:\n",
    "            temp += model[X_str_test[i][j]]\n",
    "    test_frq[i] = temp/len(X_str_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GBM_RandomSearch(X,y,Ntry,M,FixPara,RandomPara):\n",
    "    # FixPara is a dict of fixed para\n",
    "    # RandomPara is a list of tuple para for randomFun\n",
    "    \n",
    "    result = np.zeros((Ntry,5))\n",
    "    \n",
    "    for i in range(Ntry):\n",
    "        RanPara={}\n",
    "        result[i,0] = np.random.randint(*RandomPara[0])\n",
    "        RanPara['subFold'] = result[i,0]\n",
    "        \n",
    "        BasePara={}\n",
    "        result[i,1] = np.random.randint(*RandomPara[1])\n",
    "        BasePara['max_depth'] = result[i,1]\n",
    "        result[i,2] = np.random.uniform(*RandomPara[2])\n",
    "        BasePara['max_features'] = result[i,2]        \n",
    "        RanPara['BasePara']=BasePara\n",
    "        \n",
    "        result[i,3] = np.random.choice(RandomPara[3])\n",
    "        RanPara['learnRate'] = result[i,3]        \n",
    "        \n",
    "        RanPara.update(FixPara)\n",
    "        RanPara['M_est'] = int(M/RanPara['subFold'])\n",
    "        #pdb.set_trace()\n",
    "        model = GBM_KClass(**RanPara)\n",
    "        result[i,4] = model.fit(X,y)\n",
    "        \n",
    "    return pd.DataFrame(result,columns=['subFold','max_depth','max_features','learnRate','acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "randomSearch = GBM_RandomSearch(train_frq,y,20,250,{'BaseEst':ExtraTreeRegressor},\\\n",
    "                                [(2,40),(4,48),(0.05,0.5),np.logspace(-1,-5,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "randomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=LinearRegression().fit(randomSearch.iloc[:,:4].values,randomSearch.iloc[:,4].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "refit with best para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1=GBM_KClass(ExtraTreeRegressor,20,1e-4,\\\n",
    "{'max_depth':16,'splitter':'random','max_features':0.4},10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1.fit(train_frq,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model1.plot(train_frq,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='predict'></a>\n",
    "Model Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test = np.zeros((len(X_str_test),2))\n",
    "for _ in range(10):\n",
    "    for x_bat,bat in batch_iterator(X_str_test, None, model, 10, shuffle=True, sublen=1.0):\n",
    "        y_test[bat] = y_test[bat] +  predict(x_bat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test['sentiment'] = y_test[:,1]/10\n",
    "test['id'] = test.id.str.replace('\"','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test['sentiment'] = model1.predict_proba(test_frq)[:,1]\n",
    "test['id'] = test.id.str.replace('\"','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.drop('review',1)\\\n",
    "    .to_csv(r'/home/will/Desktop/data/Movie sentiment/model1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
